{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chloelbrt/Research_internship_examples/blob/main/Rapport_MI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zsDWUk5cavn"
      },
      "source": [
        "# Unsupervised Learning Tutorial of Gianni Franchi\n",
        "\n",
        "\n",
        "Welcome to ML project!\n",
        "**In this notebook, you will**:\n",
        "- Learn what is SSL\n",
        "- Learn the difficulty with Overfitting\n",
        "- Learn to implement an Convolutional Neural Network.\n",
        "- Learn to train it when we don't have enough data\n",
        "\n",
        "If you have never used jupyter notebooks, nor Colab notebooks, [here](https://colab.research.google.com/notebooks/welcome.ipynb) is a short intro.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTJ7Zz6TAaJp"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "from numpy import asarray\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.init as init\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms\n",
        "import torchvision.transforms as transforms\n",
        "import torch.utils.data as data\n",
        "import torchvision\n",
        "import random\n",
        "import math\n",
        "from torch.autograd import Variable\n",
        "#import matplotlib.pyplot as plt\n",
        "#from modules import *\n",
        "#import torchvision.models as models_pytorch\n",
        "#import h5py\n",
        "#import torch.optim as optim\n",
        "#import augmentations\n",
        "from torch.nn.functional import kl_div, softmax, log_softmax\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "from os.path import exists, join, split\n",
        "from os import listdir\n",
        "from os.path import join\n",
        "from PIL import Image, ImageFilter , ImageDraw\n",
        "import PIL\n",
        "import random\n",
        "#import madgrad \n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import svm\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "#! pip install madgrad\n",
        "#! pip install efficientnet_pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kFCoFJzWApYo"
      },
      "outputs": [],
      "source": [
        "num_class = 10 #number of classes\n",
        "num_classes=num_class\n",
        "seed=111 #seed for the algorithm\n",
        "batch_size = 32\n",
        "num_train =100 # number of training image by classe\n",
        "cutout=16  # parameter for the cutout\n",
        "num_epochs=50\n",
        "#Validation set size\n",
        "valid_size = 200\n",
        "\n",
        "#An array with the different learning rates.\n",
        "\n",
        "lrarray=[0.001, 0.01, 0.1, 0.5]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ux1VVt2AtcH"
      },
      "source": [
        "# First let us define a CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YDbhFAHJAx97"
      },
      "outputs": [],
      "source": [
        "bn_momentum = 0.9\n",
        "\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=True)\n",
        "\n",
        "\n",
        "def conv_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        init.xavier_uniform_(m.weight, gain=np.sqrt(2))\n",
        "        init.constant_(m.bias, 0)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        init.constant_(m.weight, 1)\n",
        "        init.constant_(m.bias, 0)\n",
        "\n",
        "\n",
        "class WideBasic(nn.Module):\n",
        "    def __init__(self, in_planes, planes, dropout_rate, stride=1):\n",
        "        super(WideBasic, self).__init__()\n",
        "        self.bn1 = nn.BatchNorm2d(in_planes, momentum=bn_momentum)\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, padding=1, bias=True)\n",
        "        self.dropout = nn.Dropout(p=dropout_rate)\n",
        "        self.bn2 = nn.BatchNorm2d(planes, momentum=bn_momentum)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=True)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride, bias=True),\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.dropout(self.conv1(F.relu(self.bn1(x))))\n",
        "        out = self.conv2(F.relu(self.bn2(out)))\n",
        "        out += self.shortcut(x)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class WideResNet(nn.Module):\n",
        "    def __init__(self, depth, widen_factor, dropout_rate, num_classes):\n",
        "        super(WideResNet, self).__init__()\n",
        "        self.in_planes = 16\n",
        "\n",
        "        assert ((depth - 4) % 6 == 0), 'Wide-resnet depth should be 6n+4'\n",
        "        n = int((depth - 4) / 6)\n",
        "        k = widen_factor\n",
        "\n",
        "        nStages = [16, 16*k, 32*k, 64*k]\n",
        "\n",
        "        self.conv1 = conv3x3(3, nStages[0])\n",
        "        self.layer1 = self._wide_layer(WideBasic, nStages[1], n, dropout_rate, stride=1)\n",
        "        self.layer2 = self._wide_layer(WideBasic, nStages[2], n, dropout_rate, stride=2)\n",
        "        self.layer3 = self._wide_layer(WideBasic, nStages[3], n, dropout_rate, stride=2)\n",
        "        self.bn1 = nn.BatchNorm2d(nStages[3], momentum=bn_momentum)\n",
        "        self.linear = nn.Linear(nStages[3], num_classes)\n",
        "\n",
        "        # self.apply(conv_init)\n",
        "\n",
        "    def _wide_layer(self, block, planes, num_blocks, dropout_rate, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, dropout_rate, stride))\n",
        "            self.in_planes = planes\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = F.relu(self.bn1(out))\n",
        "        # out = F.avg_pool2d(out, 8)\n",
        "        out = F.adaptive_avg_pool2d(out, (1, 1))\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        \n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Y7RH4rWA9c0"
      },
      "source": [
        "# now let us define a dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JE1_MblIA5V7"
      },
      "outputs": [],
      "source": [
        "class Dataset_sub_CIFAR(data.Dataset):\n",
        "\n",
        "    def __init__(self, data_feature, data_target,transform,phase='label'):\n",
        "        self.data_feature = data_feature\n",
        "        self.data_target = data_target\n",
        "        self.transform = transform\n",
        "        self.phase=phase\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_feature)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # load image as ndarray type (Height * Width * Channels)\n",
        "        # be carefull for converting dtype to np.uint8 [Unsigned integer (0 to 255)]\n",
        "        # in this example, i don't use ToTensor() method of torchvision.transforms\n",
        "        # so you can convert numpy ndarray shape to tensor in PyTorch (H, W, C) --> (C, H, W)\n",
        "        if self.phase=='label':\n",
        "            data_feature = self.transform(Image.fromarray(np.uint8(self.data_feature[index])))\n",
        "            data_target =  self.data_target[index]\n",
        "            return data_feature, data_target\n",
        "\n",
        "        else:\n",
        "            data_feature = self.data_feature[index].float()\n",
        "            return data_feature\n",
        "\n",
        "\n",
        "class CutoutDefault(object):\n",
        "    \"\"\"\n",
        "    Reference : https://github.com/quark0/darts/blob/master/cnn/utils.py\n",
        "    \"\"\"\n",
        "    def __init__(self, length):\n",
        "        self.length = length\n",
        "\n",
        "    def __call__(self, img):\n",
        "        if self.length <= 0:\n",
        "            return img\n",
        "        h, w = img.size(1), img.size(2)\n",
        "        mask = np.ones((h, w), np.float32)\n",
        "        y = np.random.randint(h)\n",
        "        x = np.random.randint(w)\n",
        "\n",
        "        y1 = np.clip(y - self.length // 2, 0, h)\n",
        "        y2 = np.clip(y + self.length // 2, 0, h)\n",
        "        x1 = np.clip(x - self.length // 2, 0, w)\n",
        "        x2 = np.clip(x + self.length // 2, 0, w)\n",
        "\n",
        "        mask[y1: y2, x1: x2] = 0.\n",
        "        mask = torch.from_numpy(mask)\n",
        "        mask = mask.expand_as(img)\n",
        "        img *= mask\n",
        "        return img\n",
        "\n",
        "    \n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4, padding_mode = 'reflect'),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2471, 0.2435, 0.2616)),\n",
        "    CutoutDefault(cutout),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2471, 0.2435, 0.2616)),\n",
        "])\n",
        "\n",
        "\n",
        "############################## CHOISIR entre les deux data augmentations\n",
        "res_size=224\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize([res_size, res_size]),\n",
        "    transforms.RandomCrop(224, padding=4, padding_mode = 'reflect'),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2471, 0.2435, 0.2616)),\n",
        "    CutoutDefault(cutout),\n",
        "])\n",
        "\n",
        " \n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.Resize([res_size, res_size]),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2471, 0.2435, 0.2616)),\n",
        "])\n",
        "\n",
        "#Dataset loading\n",
        "CIFAR10_train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, transform=None, download=True)\n",
        "CIFAR10_test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, transform=None, download=True)\n",
        "\n",
        "#Dataset for Resnet Alexnet WideResnet etc..\n",
        "\n",
        "np.random.seed(seed=seed)\n",
        "permuation=np.random.permutation(len(CIFAR10_train_dataset.targets))\n",
        "\n",
        "Original_train_data_x = (CIFAR10_train_dataset.data)\n",
        "Original_train_data_y = np.array(CIFAR10_train_dataset.targets)\n",
        "Original_train_data_x = Original_train_data_x[permuation]\n",
        "Original_train_data_y = Original_train_data_y[permuation]\n",
        "\n",
        "Original_test_data_x = CIFAR10_test_dataset.data\n",
        "Original_test_data_y = np.array(CIFAR10_test_dataset.targets)\n",
        "\n",
        "#Selection of 250 labeled images for training and 2000 for validation\n",
        "incr_class = torch.zeros(num_class)\n",
        "train_idx_dico = {} #labeled images index dictionnary\n",
        "\n",
        "for i in range(num_class):\n",
        "    train_idx_dico[str(i)] = []\n",
        "\n",
        "valid_idx = np.zeros(num_class * valid_size, dtype=np.int32) #validation images indexes (2000)\n",
        "incr_t = 0\n",
        "incr_v = 0\n",
        "incrtotal = 0\n",
        "\n",
        "for idx in range(len(Original_train_data_y)):\n",
        "    class_y = Original_train_data_y[idx]\n",
        "    incrtotal += 1\n",
        "\n",
        "    train_idx_dico[str(class_y)].append(idx)\n",
        "    incr_class[class_y] += 1 #count the number of image per class\n",
        "    incr_t += 1\n",
        "\n",
        "\n",
        "train_idx = np.zeros(num_class * num_train, dtype=np.int32) #train labeled images indexes (1000)\n",
        "list_train_id = []\n",
        "list_unalabel_id = []\n",
        "valid_idx = []\n",
        "unlabel_idx_dico = {}\n",
        "for i in range(num_class):\n",
        "    unlabel_idx_dico[str(i)] = []\n",
        "for i in range(num_class):\n",
        "    list_train_id = list_train_id + train_idx_dico[str(i)][0:num_train]\n",
        "    valid_idx =valid_idx + train_idx_dico[str(i)][num_train:num_train+valid_size]\n",
        "    list_unalabel_id = list_unalabel_id + train_idx_dico[str(i)][num_train+valid_size::]\n",
        "    unlabel_idx_dico[str(i)] = train_idx_dico[str(i)][num_train::]\n",
        "\n",
        "#Get labeled and unlabeled data\n",
        "\n",
        "x_train = Original_train_data_x[[int(i) for i in list_train_id]]\n",
        "y_train = Original_train_data_y[[int(i) for i in list_train_id]]\n",
        "\n",
        "x_unlabeled = Original_train_data_x[[int(i) for i in list_unalabel_id]]\n",
        "y_unlabeled = Original_train_data_y[[int(i) for i in list_unalabel_id]]\n",
        "\n",
        "#Get validation set data\n",
        "x_valid = Original_train_data_x[[int(i) for i in valid_idx]]\n",
        "y_valid = Original_train_data_y[[int(i) for i in valid_idx]]\n",
        "\n",
        "# Printing the size of the training, validation and test sets\n",
        "print('Number of training examples: ' + str(x_train.shape[0]))\n",
        "print('Number of unlabeled examples: ' + str(x_unlabeled.shape[0]))\n",
        "print('Number of validation examples: ' + str(x_valid.shape[0]))\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "\n",
        "\n",
        "#Dataloader creation\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    Dataset_sub_CIFAR(Original_test_data_x, Original_test_data_y, transform=transform_test),\n",
        "    batch_size = batch_size,\n",
        "    shuffle=False, num_workers=2)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    Dataset_sub_CIFAR(x_train, y_train, transform=transform_train),\n",
        "    batch_size=batch_size,shuffle=True, num_workers=2) #num_workers = 2 ou 1\n",
        "\n",
        "valid_loader = torch.utils.data.DataLoader(\n",
        "    Dataset_sub_CIFAR(x_valid, y_valid, transform=transform_test),\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RW-PRa3bYJMS"
      },
      "outputs": [],
      "source": [
        "#We are going to use the scikit-learn library to use their SVC and Random forest model.\n",
        "#To train the model, we need to arrange the dataset for them (they use numpy arrays in their model)\n",
        "\n",
        "\n",
        "X_valid, y_valid = [], []\n",
        "for data, labels in valid_loader:\n",
        "    X_valid.append(data.numpy())\n",
        "    y_valid.append(labels.numpy())\n",
        "X_valid = np.concatenate(X_valid)\n",
        "y_valid = np.concatenate(y_valid)\n",
        "\n",
        "X_valid = X_valid.reshape(X_valid.shape[0], -1)\n",
        "\n",
        "\n",
        "X_train, y_train = [], []\n",
        "for data, labels in train_loader:\n",
        "    X_train.append(data.numpy())\n",
        "    y_train.append(labels.numpy())\n",
        "X_train = np.concatenate(X_train)\n",
        "y_train = np.concatenate(y_train)\n",
        "\n",
        "X_train = X_train.reshape(X_train.shape[0], -1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxIA7jzZBIjP"
      },
      "source": [
        "# Now we build the CNN and the optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmqQPUHMUjzP"
      },
      "source": [
        "We will try different deep convolutional model to train with the CIFAR10 dataset :\n",
        "WideResNet, Resnet, AlexNet \n",
        "(cuda shifts the process on the GPU)\n",
        "You can also choose to work with a Random Forest or a SVC \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1jmoAxYUXYm"
      },
      "outputs": [],
      "source": [
        "#Launch this cell to work with WideResNet as the network\n",
        "\n",
        "net = WideResNet(28, 2, dropout_rate=0.0, num_classes=num_class)\n",
        "net =net.cuda()\n",
        "net_save = WideResNet(28, 2, dropout_rate=0.0, num_classes=num_class) # model where to save the results\n",
        "net_save =net_save.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "U3HwxPPIUXqd"
      },
      "outputs": [],
      "source": [
        "#Launch this cell to work with resnet18 as the network\n",
        "#By adding the parameter weights=\"IMAGENET1K_V1\" we use the pretrained weights, if we don't put anything for the parameter it starts from random weights\n",
        "weights=\"IMAGENET1K_V1\"\n",
        "# weights=None\n",
        "\n",
        "net = models.resnet18(weights=weights)\n",
        "net.fc = nn.Linear(512, num_classes)\n",
        "net = net.cuda()\n",
        "net_save = models.resnet18(weights=weights)\n",
        "net_save.fc = nn.Linear(512, num_classes)\n",
        "net_save =net_save.cuda()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uciwU-aZUXwN"
      },
      "outputs": [],
      "source": [
        "#Launch this cell to work with alexnet as the network\n",
        "#By adding the parameter weights=\"IMAGENET1K_V1\" we use the pretrained weights, if you want to use random weight change the weights variable to None\n",
        "\n",
        "weights=\"IMAGENET1K_V1\"\n",
        "# weights=None\n",
        "\n",
        "\n",
        "net = models.alexnet(weights=weights)\n",
        "net.classifier[6] = nn.Linear(4096,num_classes)\n",
        "net = net.cuda()\n",
        "\n",
        "net_save = models.alexnet(weights=weights)\n",
        "net_save.classifier[6] = nn.Linear(4096,num_classes)\n",
        "net_save =net_save.cuda()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "klXIYCo-VLKM"
      },
      "outputs": [],
      "source": [
        "#Launch this cell to work with a Random Forest as the network\n",
        "#You can modify the number of trees in the forest by changing the n_estimators parameter \n",
        "clf = RandomForestClassifier(n_estimators=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ferjcxn7VR6y"
      },
      "outputs": [],
      "source": [
        "#Launch this cell to work with a SVC model as the network\n",
        "clf = svm.SVC()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tY_jn6VVBTgk"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5HSobiPQBNjh"
      },
      "outputs": [],
      "source": [
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "def learning_rate_scheduler(init, epoch):\n",
        "    optim_factor = 0\n",
        "    if(epoch > 200):\n",
        "        optim_factor = 3\n",
        "    elif(epoch > 160):\n",
        "        optim_factor = 2\n",
        "    elif(epoch > 80):\n",
        "        optim_factor = 1\n",
        "\n",
        "    return init*math.pow(0.1, optim_factor)\n",
        "\n",
        "\n",
        "# Training \n",
        "def train(epoch,net,trainloader,log_interval=15):\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    print('\\n=> Training Epoch #%d, LR=%.4f' %(epoch, learning_rate_scheduler(lr, epoch)))\n",
        "    optimizer = optim.SGD(net.parameters(), lr=learning_rate_scheduler(lr, epoch), momentum=0.9, weight_decay=5e-4)\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        " \n",
        "        inputs, targets = inputs.cuda(), targets.cuda() # GPU settings\n",
        "        optimizer.zero_grad()\n",
        "        #inputs, targets = Variable(inputs), Variable(targets)\n",
        "        outputs = net(inputs)               # Forward Propagation\n",
        "        loss = criterion(outputs, targets)  # Loss\n",
        "        loss.backward()  # Backward Propagation\n",
        "        optimizer.step() # Optimizer update\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets.data).cpu().sum()\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print('| Epoch [%3d/%3d] Iter[%3d/%3d]\\t\\tLoss: %.4f Acc@1: %.3f%%'\n",
        "                %(epoch, num_epochs, batch_idx+1,\n",
        "                    (len(trainloader.dataset)//batch_size)+1, loss.item(), 100.*correct/total))\n",
        "\n",
        "\n",
        "def test(epoch,net,testloader):\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            \n",
        "            inputs, targets = inputs.cuda(), targets.cuda()\n",
        "            inputs, targets = Variable(inputs), Variable(targets)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets.data).cpu().sum()\n",
        "\n",
        "        \n",
        "    acc = 100.*correct/total\n",
        "    print(\"\\n| Validation Epoch #%d\\t\\t\\tLoss: %.4f Acc@1: %.2f%%\" %(epoch, loss.item(), acc))\n",
        "    return acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jpnbckfWBWbB"
      },
      "outputs": [],
      "source": [
        "#Training for the CNN models\n",
        "\n",
        "best_acc=0\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    train(epoch,net,train_loader)\n",
        "    acc = test(epoch,net,valid_loader)\n",
        "    # Save checkpoint when best model\n",
        "    if acc > best_acc:\n",
        "        print('| Saving Best model...\\t\\t\\tTop1 = %.2f%%' %(acc))\n",
        "        net_save.load_state_dict(net.state_dict(), strict=True)\n",
        "        best_acc=acc\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CtY7lIayMzN0"
      },
      "outputs": [],
      "source": [
        "#Training for the SVM or Random Forest\n",
        "clf.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weX9mgbZBiNO"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZDqFxbtmBkAL"
      },
      "outputs": [],
      "source": [
        "#Evaluation for the CNN models \n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def test_final(net,testloader):\n",
        "    global best_acc\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            \n",
        "            inputs, targets = inputs.cuda(), targets.cuda()\n",
        "            inputs, targets = Variable(inputs), Variable(targets)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets.data).cpu().sum()\n",
        "            if batch_idx == 0:\n",
        "                predicted_concat = predicted.clone()\n",
        "            else:\n",
        "                predicted_concat = torch.cat((predicted_concat, predicted), 0)\n",
        "\n",
        "        # Save checkpoint when best model\n",
        "    acc = 100.*correct/total\n",
        "    print(\"\\n| TEST \\t\\t\\tLoss: %.4f Acc@1: %.2f%%\" %( loss.item(), acc))\n",
        "    return predicted_concat.cpu().numpy()\n",
        "    \n",
        "\n",
        "predicted_concat = test_final(net,test_loader)\n",
        "\n",
        "\n",
        "id_concat =range(len(predicted_concat))\n",
        "my_submission = pd.DataFrame({'Id': id_concat,'Expected': predicted_concat})\n",
        "\n",
        "# you could use any filename. We choose submission here\n",
        "my_submission.to_csv('submission2.csv', index=False)\n",
        "print('we have saved the submission !! ')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ofmhJGIRnLzt"
      },
      "outputs": [],
      "source": [
        "# Predict on the test data for the Random Forest or the SVM \n",
        "y_pred = clf.predict(X_valid)\n",
        "\n",
        "# Calculate accuracy\n",
        "acc = accuracy_score(y_valid, y_pred)\n",
        "print(\"Accuracy:\", acc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ij_IBcJMd5mz"
      },
      "source": [
        "#FixMatch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u44CLkvnd7M9"
      },
      "outputs": [],
      "source": [
        "\n",
        "n_classes = 10\n",
        "batch_size = 32\n",
        "epochs = 50\n",
        "train_class_size = 100\n",
        "valid_class_size = 200\n",
        "unlabeled_ratio = 7\n",
        "confidence_threshold = 0.6\n",
        "\n",
        "#set sizes\n",
        "valid_size = n_classes * valid_class_size\n",
        "labeled_size = n_classes * train_class_size\n",
        "unlabeled_size = labeled_size * unlabeled_ratio\n",
        "\n",
        "#load datasets\n",
        "print(\"Loading datasets...\")\n",
        "train_ds = torchvision.datasets.CIFAR10(root=\"./cifar10\", train=True,  download=True)\n",
        "valid_ds = torchvision.datasets.CIFAR10(root=\"./cifar10\", train=False, download=True)\n",
        "\n",
        "\n",
        "#select random data\n",
        "print(\"Generating permutations...\")\n",
        "rng = np.random.default_rng()\n",
        "valid_ids     = rng.permutation(valid_ds.data.shape[0])[:valid_size].astype(int)\n",
        "labeled_ids   = rng.permutation(train_ds.data.shape[0])[:labeled_size].astype(int)\n",
        "unlabeled_ids = rng.permutation(train_ds.data.shape[0])[:unlabeled_size].astype(int)\n",
        "\n",
        "#extract validation and training sets from datasets\n",
        "print(\"Extracting datasets...\")\n",
        "x_valid = np.array(valid_ds.data)[valid_ids]\n",
        "y_valid = np.array(valid_ds.targets)[valid_ids]\n",
        "x_labeled = np.array(train_ds.data)[labeled_ids]\n",
        "y_labeled = np.array(train_ds.targets)[labeled_ids]\n",
        "x_unlabeled = np.array(train_ds.data)[unlabeled_ids]\n",
        "\n",
        "#creating dataloaders\n",
        "print(\"Creating dataloaders...\")\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    Dataset_sub_CIFAR(Original_test_data_x, Original_test_data_y, transform=transform_test),\n",
        "    batch_size = batch_size,shuffle=False, num_workers=2)\n",
        "labeled_loader = torch.utils.data.DataLoader(\n",
        "    Dataset_sub_CIFAR(x_labeled, y_labeled, transform=transform_train),\n",
        "    batch_size=batch_size,shuffle=True, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vT89Go-5qI-R"
      },
      "outputs": [],
      "source": [
        "# Training \n",
        "def training_epoch(epoch,net,loader,log_interval=15):\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    print('\\n=> Training Epoch #%d, LR=%.4f' %(epoch, learning_rate_scheduler(lr, epoch)))\n",
        "    optimizer = optim.SGD(net.parameters(), lr=learning_rate_scheduler(lr, epoch), momentum=0.9, weight_decay=5e-4)\n",
        "    for batch_idx, (inputs, targets) in enumerate(loader):\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets.data).cpu().sum()\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print('| Epoch [%3d/%3d] Iter[%3d/%3d]\\t\\tLoss: %.4f Acc@1: %.3f%%'\n",
        "                %(epoch, num_epochs, batch_idx+1,\n",
        "                    (len(trainloader.dataset)//batch_size)+1, loss.item(), 100.*correct/total))\n",
        "\n",
        "\n",
        "def test(epoch,net,testloader):\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            \n",
        "            inputs, targets = inputs.cuda(), targets.cuda()\n",
        "            inputs, targets = Variable(inputs), Variable(targets)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets.data).cpu().sum()\n",
        "\n",
        "        \n",
        "    acc = 100.*correct/total\n",
        "    print(\"\\n| Validation Epoch #%d\\t\\t\\tLoss: %.4f Acc@1: %.2f%%\" %(epoch, loss.item(), acc))\n",
        "    return acc\n",
        "\n",
        "def learning_rate_scheduler(init, epoch):\n",
        "    optim_factor = 0\n",
        "    if(epoch > 200):\n",
        "        optim_factor = 3\n",
        "    elif(epoch > 160):\n",
        "        optim_factor = 2\n",
        "    elif(epoch > 80):\n",
        "        optim_factor = 1\n",
        "    return init*math.pow(0.1, optim_factor)\n",
        "\n",
        "def pseudo_label(net, unlabeled, threshold=0.5):\n",
        "  n = unlabeled.size(0)\n",
        "  labels = np.zeros(n)\n",
        "  label_count = 0\n",
        "  predictions = net(unlabeled)\n",
        "  for i in range(n):\n",
        "    probs = torch.functional.softmax(predictions[i])\n",
        "    confidence = torch.max(prob)\n",
        "    if confidence > threshold:\n",
        "      labels[i] = [1 if p == confidence else 0 for p in prob]\n",
        "  return labels\n",
        "\n",
        "\n",
        "def pseudolabeled_loader(unlabeled, labels):\n",
        "  count = torch.count_nonzero(labels)\n",
        "  xy_index = 0\n",
        "  nonzeros = torch.nonzero(torch.sum(labels,dim=2))\n",
        "  n = nonzeros.size(0)\n",
        "  x = torch.zeros((n,*unlabeled[0].size))\n",
        "  y = torch.zeros((n,n_classes))\n",
        "  for i in nonzeros:\n",
        "    x[xy_index] = unlabeled[i]\n",
        "    y[xy_index] = labels[i]\n",
        "  return torch.utils.data.DataLoader(Dataset_sub_CIFAR(x, y), batch_size = batch_size,shuffle=False, num_workers=2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ho830Yx6fK0_"
      },
      "outputs": [],
      "source": [
        "net = WideResNet(28, 2, dropout_rate=0.0, num_classes=num_class)\n",
        "net = net.cuda()\n",
        "net_save = WideResNet(28, 2, dropout_rate=0.0, num_classes=num_class) # model where to save the results\n",
        "net_save = net_save.cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "weak_augment = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.Resize(32),\n",
        "    transforms.RandomCrop(size=32,padding=int(32*0.125),padding_mode='reflect')\n",
        "])\n",
        "\n",
        "def strong_augment(n_augments=2,magnitude=0.5):\n",
        "  return transforms.Compose([\n",
        "      transforms.RandomErasing(p=1,ratio=(1,1),scale=(0.1,0.1),value=127),\n",
        "      transforms.RandAugment(n_augments,magnitude) ])\n",
        "\n",
        "#Labeled training\n",
        "net.train()\n",
        "for epoch in range(epochs):\n",
        "  training_epoch(epoch, net, labeled_loader)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  weak_augments = weak_augment(x_unlabeled)\n",
        "  pseudolabels = pseudo_label(net, weak_augments, 0.6)\n",
        "  strong_augments = strong_augment()(x_unlabeled)\n",
        "  loader = pseudolabeled_loader(strong_augments, pseudolabels)\n",
        "  training_epoch(epoch, net, pseudolabeled_loader)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrJS4AqABprt"
      },
      "source": [
        "# Question for the report\n",
        "I want that you send me a small report with the answer to this question and your notebook.\n",
        "- Q0: Please train wideresnet, and please understand a bit wideresnet.\n",
        "- Q1: Please change DNN with a Resnet 18. Try with one that is pre-trained and one that is not pre-trained. \n",
        "- Q2: Please change DNN with an AlexNet. Try with one that is pre-trained and one that is not pre-trained. (Be careful, you need a bit to play with the learning rate, for questions two, one and zeros I want to see the training loss and training accuracy. What other curb is interesting? Plot it and analyse it.)\n",
        "- Q3: Please try to train an SVM and a random forest.\n",
        "- Q4 After you have trained several models please draw a table and make some conclusions.\n",
        "-  Q5 Read the paper fixmatch (https://amitness.com/2020/03/fixmatch-semi-supervised/) and explain it.\n",
        "- Q6 please try to implement it and try to make it work.\n",
        "- Q7 What can we do to avoid overfitting in Deep learning?\n",
        "\n",
        "\n",
        "*Q0-Q5 = 14 pts*\n",
        "\n",
        "*Q6 = 6 pts*\n",
        "\n",
        "*Q7 = 1 pts*"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "jxIA7jzZBIjP",
        "tY_jn6VVBTgk",
        "weX9mgbZBiNO"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}